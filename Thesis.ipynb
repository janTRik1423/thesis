{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "79f537b5",
   "metadata": {},
   "source": [
    "# Dust Detection from Remote Sensing Images\n",
    "In this notebook input images are used from MODIS Terra satellite from NASA. Here one sample .hdf file is used for input.\n",
    "There is one data folder where the .hdf file is located. That file is downloaded from NASA LAADS DAAC."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47cd6799",
   "metadata": {},
   "source": [
    "##### All the Necessary packages and libraries are loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fa68f532",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Enabling eager execution\n",
      "INFO:tensorflow:Enabling v2 tensorshape\n",
      "INFO:tensorflow:Enabling resource variables\n",
      "INFO:tensorflow:Enabling tensor equality\n",
      "INFO:tensorflow:Enabling control flow v2\n"
     ]
    }
   ],
   "source": [
    "from scipy.signal import cspline2d\n",
    "\n",
    "import imageio as img\n",
    "import cv2\n",
    "from PIL import Image\n",
    "from skimage import io\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from satpy import Scene, MultiScene, available_readers, available_writers, find_files_and_readers\n",
    "from satpy.readers import modis_l1b, modis_l2\n",
    "from satpy.writers import get_enhanced_image\n",
    "from satpy.composites import GenericCompositor\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pyresample import geometry\n",
    "\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import earthpy as et\n",
    "import earthpy.plot as ep\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "from skimage.io import imread, imshow, concatenate_images\n",
    "from skimage.transform import resize\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import Input, BatchNormalization, Activation, Dense, Dropout\n",
    "from keras.layers.convolutional import Conv2D, Conv2DTranspose\n",
    "from keras.layers.pooling import MaxPooling2D\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from keras.preprocessing.image import img_to_array, load_img\n",
    "\n",
    "import tensorflow as tf\n",
    "# import statements\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils import data\n",
    "from torchvision import datasets, transforms, models\n",
    "import argparse\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import torchvision\n",
    "\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "# From local helper files\n",
    "from helper_evaluation import set_all_seeds, set_deterministic, compute_confusion_matrix\n",
    "from helper_train import train_model\n",
    "from helper_plotting import plot_training_loss, plot_accuracy, show_examples, plot_confusion_matrix\n",
    "from helper_dataset import UnNormalize\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# from utils import load_checkpoint, save_checkpoint, ensure_dir\n",
    "# from model import MyModel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c907c38",
   "metadata": {},
   "source": [
    "## Module 1: Remote Sensing MODIS Data\n",
    "##### Checking the directory to find the location of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1744ea30",
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################\n",
    "# LOADING DATASET\n",
    "#####################\n",
    "\n",
    "def content_from_path(path):\n",
    "    return os.listdir(path)\n",
    "\n",
    "def generate_data_from_path(path):\n",
    "    images = []\n",
    "    dataset_path = content_from_path(path)\n",
    "    \n",
    "    for item in dataset_path:\n",
    "        images.append((\"Yes\", item))\n",
    "        \n",
    "    return images\n",
    "     \n",
    "def generate_data_from_folder():\n",
    "    images = []\n",
    "    \n",
    "    for item in dataset_path:\n",
    "        path = 'nasa' + '/' + item\n",
    "        if os.path.isdir(path):  \n",
    "            image = os.listdir(path)\n",
    "            images.append((\"Yes\", str(path + '/' + image[0])))\n",
    "    return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "204fba24",
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################\n",
    "# MAKING DATAFRAME\n",
    "#####################\n",
    "\n",
    "images = generate_data_from_path('nasa-calibrated-resize');\n",
    "dust_df = pd.DataFrame(data=images, columns=[\"Dust\", \"image\"])\n",
    "dust_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8806669",
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################\n",
    "# MAKING HORIZONTAL SPLITTED DATASET\n",
    "##################################################\n",
    "\n",
    "counter = 0\n",
    "for item in dust_df['image']:\n",
    "    \n",
    "    input_path = \"nasa-calibrated-resize/\" + item\n",
    "    if input_path.endswith('DS_Store') or os.path.isdir(input_path):\n",
    "        continue\n",
    "    outputPath = \"horizontal/\"\n",
    "    \n",
    "    img = cv2.imread(input_path)\n",
    "    im = Image.open(input_path)\n",
    "    x_width, y_height = im.size\n",
    "    # print(x_width, y_height)\n",
    "    \n",
    "    outputFileFormat = \"{0}-{1}.jpg\"\n",
    "    baseName = \"cropped\"\n",
    "    \n",
    "    split = 10\n",
    "    edges = np.linspace(0, x_width, split + 1)\n",
    "    # print(edges)\n",
    "    # print(edges[:-1])\n",
    "    # print(edges[1:])\n",
    "    for start, end in zip(edges[:-1], edges[1:]):\n",
    "        box = (start, 0, end, y_height)\n",
    "        a = im.crop(box)\n",
    "        x_width, y_height = a.size\n",
    "        # print(x_width, y_height)\n",
    "        a.load()\n",
    "        outputName = os.path.join(outputPath, outputFileFormat.format(baseName, counter + 1))\n",
    "        counter = counter + 1\n",
    "        # print(outputName)\n",
    "        a.save(outputName, \"JPEG\")\n",
    "        \n",
    "horizontal_images = generate_data_from_path('horizontal');\n",
    "dust_df_horizontal = pd.DataFrame(data=horizontal_images, columns=[\"Dust\", \"image\"])\n",
    "dust_df_horizontal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a427df64",
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in dust_df_horizontal['image']:\n",
    "    \n",
    "    input_path = \"horizontal/\" + item\n",
    "    if input_path.endswith('DS_Store') or os.path.isdir(input_path):\n",
    "        continue\n",
    "    \n",
    "    img = cv2.imread(input_path)\n",
    "    im = Image.open(input_path)\n",
    "    # im.show()\n",
    "    x_width, y_height = im.size\n",
    "    print(x_width, y_height)\n",
    "    # print(img.shape)\n",
    "    # print(input_path)\n",
    "    if x_width != 128:\n",
    "        print(input_path)\n",
    "        os.remove(input_path)\n",
    "    # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dd6cf43",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b552046",
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################\n",
    "# MAKING VERTICAL SPLITTED DATASET\n",
    "##################################################\n",
    "\n",
    "counter = 0\n",
    "for item in dust_df_horizontal['image']:\n",
    "    \n",
    "    input_path = \"horizontal/\" + item\n",
    "    if input_path.endswith('DS_Store') or os.path.isdir(input_path):\n",
    "        continue\n",
    "    outputPath = \"horizontal/final\"\n",
    "    \n",
    "    img = cv2.imread(input_path)\n",
    "    im = Image.open(input_path)\n",
    "    x_width, y_height = im.size\n",
    "    # print(x_width, y_height)\n",
    "    \n",
    "    outputFileFormat = \"{0}-{1}.jpg\"\n",
    "    baseName = \"cropped\"\n",
    "    \n",
    "    split = 15\n",
    "    edges = np.linspace(0, y_height, split + 1)\n",
    "    # print(edges)\n",
    "    # print(edges[:-1])\n",
    "    # print(edges[1:])\n",
    "    for start, end in zip(edges[:-1], edges[1:]):\n",
    "        box = (0, start, x_width, end)\n",
    "        a = im.crop(box)\n",
    "        a.load()\n",
    "        outputName = os.path.join(outputPath, outputFileFormat.format(baseName, counter + 1))\n",
    "        counter = counter + 1\n",
    "        # print(outputName)\n",
    "        a.save(outputName, \"JPEG\")\n",
    "        \n",
    "final_images = generate_data_from_path('horizontal/final');\n",
    "dust_df_final = pd.DataFrame(data=final_images, columns=[\"Dust\", \"image\"])\n",
    "dust_df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d7daa6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in dust_df_final['image']:\n",
    "    \n",
    "    input_path = \"horizontal/final/\" + item\n",
    "    if input_path.endswith('DS_Store') or os.path.isdir(input_path):\n",
    "        continue\n",
    "    count = 1\n",
    "    img = cv2.imread(input_path)\n",
    "    im = Image.open(input_path)\n",
    "    # im.show()\n",
    "    x_width, y_height = im.size\n",
    "    print(x_width, y_height)\n",
    "    # print(item.index)\n",
    "    if y_height != 128:\n",
    "        print(img.shape)\n",
    "        os.remove(input_path)\n",
    "    # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "adc6ef35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dust</th>\n",
       "      <th>image</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Yes</td>\n",
       "      <td>cropped-15032.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Yes</td>\n",
       "      <td>cropped-8525.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Yes</td>\n",
       "      <td>cropped-7616.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Yes</td>\n",
       "      <td>cropped-6508.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Yes</td>\n",
       "      <td>cropped-995.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19136</th>\n",
       "      <td>Yes</td>\n",
       "      <td>cropped-9622.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19137</th>\n",
       "      <td>Yes</td>\n",
       "      <td>cropped-6511.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19138</th>\n",
       "      <td>Yes</td>\n",
       "      <td>cropped-14335.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19139</th>\n",
       "      <td>Yes</td>\n",
       "      <td>cropped-754.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19140</th>\n",
       "      <td>Yes</td>\n",
       "      <td>cropped-12744.jpg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>19141 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Dust              image\n",
       "0      Yes  cropped-15032.jpg\n",
       "1      Yes   cropped-8525.jpg\n",
       "2      Yes   cropped-7616.jpg\n",
       "3      Yes   cropped-6508.jpg\n",
       "4      Yes    cropped-995.jpg\n",
       "...    ...                ...\n",
       "19136  Yes   cropped-9622.jpg\n",
       "19137  Yes   cropped-6511.jpg\n",
       "19138  Yes  cropped-14335.jpg\n",
       "19139  Yes    cropped-754.jpg\n",
       "19140  Yes  cropped-12744.jpg\n",
       "\n",
       "[19141 rows x 2 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##########################################\n",
    "# MAKING FINAL DATAFRAME\n",
    "##########################################\n",
    "\n",
    "images = generate_data_from_path('horizontal/final/');\n",
    "dust_df_final_calibrated = pd.DataFrame(data=images, columns=[\"Dust\", \"image\"])\n",
    "dust_df_final_calibrated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "858dcc68",
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in dust_df_final_calibrated['image']:\n",
    "    \n",
    "    input_path = \"horizontal/final/\" + item\n",
    "    if input_path.endswith('DS_Store') or os.path.isdir(input_path):\n",
    "        continue\n",
    "    count = 1\n",
    "    img = cv2.imread(input_path)\n",
    "    im = Image.open(input_path)\n",
    "    # im.show()\n",
    "    x_width, y_height = im.size\n",
    "    print(x_width, y_height)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dff06766-a1d5-4043-9d82-ad54d7246cd4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##########################\n",
    "### SETTINGS\n",
    "##########################\n",
    "\n",
    "RANDOM_SEED = 123\n",
    "BATCH_SIZE = 256\n",
    "NUM_EPOCHS = 15\n",
    "DEVICE = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0203902f-7a6d-46ee-9ab4-8d1b2bce89ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DustDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, csv_file, root_dir, transform=None):\n",
    "        self.annotations = pd.read_csv(csv_file)\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.annotations)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img_path = os.path.join(self.root_dir, self.annotations.iloc[index, 0])\n",
    "        image = io.imread(img_path)\n",
    "        y_label = torch.tensor(int(self.annotations.iloc[index, 1]))\n",
    "\n",
    "        # print(img_path)\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return (image, y_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "223dd4f6-c1bb-435e-ad6e-b1478ea03d14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image batch dimensions: torch.Size([256, 3, 128, 128])\n",
      "Image label dimensions: torch.Size([256])\n",
      "Class labels of 10 examples: tensor([0, 0, 1, 0, 0, 0, 0, 0, 1, 0])\n"
     ]
    }
   ],
   "source": [
    "##########################\n",
    "### FINAL DUST DATASET\n",
    "##########################\n",
    "\n",
    "# resize_transform = torchvision.transforms.Compose(\n",
    "#     [torchvision.transforms.Resize((32, 32)),\n",
    "#      torchvision.transforms.ToTensor(),\n",
    "#      torchvision.transforms.Normalize((0.5,), (0.5,))])\n",
    "\n",
    "train_transforms = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "                                      ])\n",
    "\n",
    "test_transforms = torchvision.transforms.Compose([       \n",
    "    torchvision.transforms.ToTensor(),                \n",
    "    torchvision.transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "# Load Data\n",
    "dataset = DustDataset(\n",
    "    csv_file=\"dataset.csv\",\n",
    "    root_dir=\"horizontal/dataset/\",\n",
    "    transform=train_transforms,\n",
    ")\n",
    "\n",
    "train_set, valid_set, test_set = torch.utils.data.random_split(dataset, [256, 30, 33])\n",
    "train_loader = DataLoader(dataset=train_set, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_set, batch_size=BATCH_SIZE, shuffle=True)\n",
    "valid_loader = DataLoader(dataset=valid_set, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "# Checking the dataset\n",
    "for images, labels in train_loader:  \n",
    "    print('Image batch dimensions:', images.shape)\n",
    "    print('Image label dimensions:', labels.shape)\n",
    "    print('Class labels of 10 examples:', labels[:10])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a13eacef-51ba-4684-926f-1f606c55f67f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlexNetDust(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "        self.features = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(3, 64, kernel_size=11, stride=4, padding=2),\n",
    "            torch.nn.ReLU(inplace=True),\n",
    "            torch.nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            torch.nn.Conv2d(64, 192, kernel_size=5, padding=2),\n",
    "            torch.nn.ReLU(inplace=True),\n",
    "            torch.nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            torch.nn.Conv2d(192, 384, kernel_size=3, padding=1),\n",
    "            torch.nn.ReLU(inplace=True),\n",
    "            torch.nn.Conv2d(384, 256, kernel_size=3, padding=1),\n",
    "            torch.nn.ReLU(inplace=True),\n",
    "            torch.nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            torch.nn.ReLU(inplace=True),\n",
    "            torch.nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "        )\n",
    "        self.avgpool = torch.nn.AdaptiveAvgPool2d((6, 6))\n",
    "        self.classifier = torch.nn.Sequential(\n",
    "            torch.nn.Dropout(0.5),\n",
    "            torch.nn.Linear(256 * 6 * 6, 4096),\n",
    "            torch.nn.ReLU(inplace=True),\n",
    "            torch.nn.Dropout(0.5),\n",
    "            torch.nn.Linear(4096, 4096),\n",
    "            torch.nn.ReLU(inplace=True),\n",
    "            torch.nn.Linear(4096, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = x.view(x.size(0), 256 * 6 * 6)\n",
    "        logits = self.classifier(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "941b8c90-8ab2-4bc1-81ed-5008a29d9442",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/rafi/lib/python3.9/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  ../c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001/015 | Batch 0000/0001 | Loss: 0.6909\n",
      "Epoch: 001/015 | Train: 81.25% | Validation: 73.33%\n",
      "Time elapsed: 0.14 min\n",
      "Epoch: 002/015 | Batch 0000/0001 | Loss: 0.6627\n",
      "Epoch: 002/015 | Train: 81.25% | Validation: 73.33%\n",
      "Time elapsed: 0.28 min\n",
      "Epoch: 003/015 | Batch 0000/0001 | Loss: 0.6168\n",
      "Epoch: 003/015 | Train: 81.25% | Validation: 73.33%\n",
      "Time elapsed: 0.41 min\n",
      "Epoch: 004/015 | Batch 0000/0001 | Loss: 0.5646\n",
      "Epoch: 004/015 | Train: 81.25% | Validation: 73.33%\n",
      "Time elapsed: 0.54 min\n",
      "Epoch: 005/015 | Batch 0000/0001 | Loss: 0.5160\n",
      "Epoch: 005/015 | Train: 81.25% | Validation: 73.33%\n",
      "Time elapsed: 0.67 min\n",
      "Epoch: 006/015 | Batch 0000/0001 | Loss: 0.4764\n",
      "Epoch: 006/015 | Train: 81.25% | Validation: 73.33%\n",
      "Time elapsed: 0.80 min\n",
      "Epoch: 007/015 | Batch 0000/0001 | Loss: 0.4521\n",
      "Epoch: 007/015 | Train: 81.25% | Validation: 73.33%\n",
      "Time elapsed: 0.93 min\n",
      "Epoch: 008/015 | Batch 0000/0001 | Loss: 0.4728\n",
      "Epoch: 008/015 | Train: 81.25% | Validation: 73.33%\n",
      "Time elapsed: 1.06 min\n",
      "Epoch: 009/015 | Batch 0000/0001 | Loss: 0.5142\n",
      "Epoch: 009/015 | Train: 81.25% | Validation: 73.33%\n",
      "Time elapsed: 1.19 min\n",
      "Epoch: 010/015 | Batch 0000/0001 | Loss: 0.4849\n",
      "Epoch: 010/015 | Train: 81.25% | Validation: 73.33%\n",
      "Time elapsed: 1.32 min\n",
      "Epoch: 011/015 | Batch 0000/0001 | Loss: 0.4685\n",
      "Epoch: 011/015 | Train: 81.25% | Validation: 73.33%\n",
      "Time elapsed: 1.46 min\n",
      "Epoch: 012/015 | Batch 0000/0001 | Loss: 0.4696\n",
      "Epoch: 012/015 | Train: 81.25% | Validation: 73.33%\n",
      "Time elapsed: 1.59 min\n",
      "Epoch    12: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Epoch: 013/015 | Batch 0000/0001 | Loss: 0.4716\n",
      "Epoch: 013/015 | Train: 81.25% | Validation: 73.33%\n",
      "Time elapsed: 1.72 min\n",
      "Epoch: 014/015 | Batch 0000/0001 | Loss: 0.4711\n",
      "Epoch: 014/015 | Train: 81.25% | Validation: 73.33%\n",
      "Time elapsed: 1.85 min\n"
     ]
    }
   ],
   "source": [
    "model = AlexNetDust(num_classes=2)\n",
    "\n",
    "model = model.to(DEVICE)\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), momentum=0.9, lr=0.1)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer,\n",
    "                                                       factor=0.1,\n",
    "                                                       mode='max',\n",
    "                                                       verbose=True)\n",
    "\n",
    "minibatch_loss_list, train_acc_list, valid_acc_list = train_model(\n",
    "    model=model,\n",
    "    num_epochs=NUM_EPOCHS,\n",
    "    train_loader=train_loader,\n",
    "    valid_loader=valid_loader,\n",
    "    test_loader=test_loader,\n",
    "    optimizer=optimizer,\n",
    "    device=DEVICE,\n",
    "    scheduler=scheduler,\n",
    "    scheduler_on='valid_acc',\n",
    "    logging_interval=100)\n",
    "\n",
    "plot_training_loss(minibatch_loss_list=minibatch_loss_list,\n",
    "                   num_epochs=NUM_EPOCHS,\n",
    "                   iter_per_epoch=len(train_loader),\n",
    "                   results_dir=None,\n",
    "                   averaging_iterations=200)\n",
    "plt.show()\n",
    "\n",
    "plot_accuracy(train_acc_list=train_acc_list,\n",
    "              valid_acc_list=valid_acc_list,\n",
    "              results_dir=None)\n",
    "plt.ylim([60, 100])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97c93fdc",
   "metadata": {},
   "source": [
    "##### Changing the folder to data to get access of the .hdf file\n",
    "This file is downloaded before from NASA LAADS DAAC. Changing the directory should be executed only once otherwise it will show error because it has already changed the directory.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95d32e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set working directory to data to get access of the file\n",
    "# os.chdir(os.path.join(os.getcwd(), 'data'))\n",
    "\n",
    "# Accessing file for processing\n",
    "filename = os.path.join(\"MOD021KM.A2021092.0020.006.2021092134055.hdf\")\n",
    "filenames = [filename]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "208177f4",
   "metadata": {},
   "source": [
    "##### Loading data to scene object from SatPy. \n",
    "SatPy is used here for processing remote sensing images\n",
    "Printing available dataset names which is required for listing the bands\n",
    "Here total 36 bands information are available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d19114",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODIS scene object using the file retrieved from data folder\n",
    "modis_scene = Scene(reader='modis_l1b', filenames=filenames)\n",
    "modis_scene.available_dataset_names()\n",
    "# modis_scene.unload"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ef3421c",
   "metadata": {},
   "source": [
    "##### Method: Band Details and Plotting\n",
    "This is the custom method which takes band no and color map as parameter and shows all the necessary information about that particular band. The second method plot_band() takes the band no as parameter and plot the band."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c095740c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def band_details(band_no, cmap):\n",
    "    \n",
    "    print(\"Band no: \", band_no)\n",
    "    print(\"Platform name: \", modis_scene[band_no].attrs['platform_name'])\n",
    "    print(\"Dimension: \", modis_scene[band_no].dims)\n",
    "    print(\"No of dimension: \", modis_scene[band_no].ndim)\n",
    "    print(\"Wavelength: \", modis_scene[band_no].wavelength)\n",
    "    print(\"Calibration: \", modis_scene[band_no].calibration)\n",
    "    print(\"Maximum value: \", modis_scene[band_no].max().values)\n",
    "    \n",
    "    modis_scene[band_no].plot.imshow(cmap=cmap)\n",
    "    plt.title(\"Band-{}\".format(band_no))\n",
    "\n",
    "def plot_band(band_no):\n",
    "\n",
    "    plt.figure()\n",
    "    modis_scene.load([band_no])\n",
    "    band_details(band_no, 'cividis')\n",
    "    modis_scene[band_no]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cdc940f",
   "metadata": {},
   "outputs": [],
   "source": [
    "band_no = '3'\n",
    "plot_band(band_no)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0159231b",
   "metadata": {},
   "source": [
    "### Data Extraction\n",
    "Image data is extrcted from the metadata of MODIS hdf file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a4f47cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = modis_scene[band_no]\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61fec55e",
   "metadata": {},
   "source": [
    "### Resizing Image\n",
    "The dimension of input image is too high so the reduced dimension of image is used by resizing it to (128, 128) where original image dimension was (2030, 1354). Finally image value is normalized by dividing by 255. Image is first converted to matrix and then again to numpy array to match the dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ed99b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "iteration = 100\n",
    "\n",
    "image = resize(img_to_array(modis_scene[band_no]), (128, 128),  mode = 'constant', preserve_range = True)\n",
    "image = image/255.0\n",
    "image.shape\n",
    "image = np.matrix(image)\n",
    "image = np.array(image)\n",
    "image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bffa10b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the image after resizing\n",
    "plt.figure()\n",
    "plt.imshow(image, cmap = 'viridis')\n",
    "plt.title('Band - ' + band_no)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79cf460e",
   "metadata": {},
   "outputs": [],
   "source": [
    "modis_scene.unload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bce11fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
